{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Dependancy Imports"
      ],
      "metadata": {
        "id": "cAgOa605Cxw0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "F3xDFaWbCsv7"
      },
      "outputs": [],
      "source": [
        "# Basic imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# Specific imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Data"
      ],
      "metadata": {
        "id": "O0d2jQjVC2vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign data to variable\n",
        "dataset = pd.read_csv('HousingData.csv')\n",
        "dataset = dataset.head(229)\n",
        "\n",
        "# Convert the date strings into datetime objects\n",
        "dataset['Date'] = pd.to_datetime(dataset['Date'])\n",
        "\n",
        "# Convert Date Features to a number format\n",
        "dataset['Year'] = dataset['Date'].dt.year\n",
        "dataset['Month'] = dataset['Date'].dt.month\n",
        "\n",
        "# Take a small look at the dataframe produced\n",
        "print(dataset.head())\n",
        "print(dataset.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiOD-yHkC5Hx",
        "outputId": "a267ba7a-9fac-4b53-f317-22d6f3046ed2"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Date  Composite_HPI  Single_Family_HPI  One_Storey_HPI  \\\n",
            "0 2005-01-01          100.0              100.0           100.0   \n",
            "1 2005-02-01          101.0              101.0           101.2   \n",
            "2 2005-03-01          102.1              102.1           102.4   \n",
            "3 2005-04-01          103.1              103.2           103.8   \n",
            "4 2005-05-01          103.8              103.8           104.6   \n",
            "\n",
            "   Two_Storey_HPI  Townhouse_HPI  Apartment_HPI  Composite_Benchmark  \\\n",
            "0           100.0          100.0          100.0             237700.0   \n",
            "1           100.9          100.9          101.3             240100.0   \n",
            "2           101.9          101.7          102.3             242600.0   \n",
            "3           102.9          102.5          103.1             245100.0   \n",
            "4           103.4          103.2          104.1             246700.0   \n",
            "\n",
            "   Single_Family_Benchmark  One_Storey_Benchmark  Two_Storey_Benchmark  \\\n",
            "0                 257600.0              205000.0              300000.0   \n",
            "1                 260100.0              207400.0              302700.0   \n",
            "2                 263000.0              210000.0              305700.0   \n",
            "3                 265800.0              212700.0              308600.0   \n",
            "4                 267500.0              214500.0              310100.0   \n",
            "\n",
            "   Townhouse_Benchmark  Apartment_Benchmark  Year  Month  \n",
            "0             199600.0             172400.0  2005      1  \n",
            "1             201400.0             174600.0  2005      2  \n",
            "2             202900.0             176400.0  2005      3  \n",
            "3             204500.0             177800.0  2005      4  \n",
            "4             205900.0             179500.0  2005      5  \n",
            "(229, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data in to features"
      ],
      "metadata": {
        "id": "kcRxKA-aC8VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into X and y sets\n",
        "X = dataset[[\n",
        "    'Year',\n",
        "    'Month',\n",
        "    'Composite_HPI',\n",
        "    'Single_Family_Benchmark',\n",
        "    'One_Storey_Benchmark',\n",
        "    'Two_Storey_Benchmark',\n",
        "    'Townhouse_Benchmark',\n",
        "    'Apartment_Benchmark'\n",
        "    ]]\n",
        "\n",
        "y = dataset['Composite_Benchmark']"
      ],
      "metadata": {
        "id": "b6C9NXAPC-59"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the dataset"
      ],
      "metadata": {
        "id": "DPYHh1Z4Dx-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your sets into train and test variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size = 0.2,\n",
        "    random_state = 0\n",
        "    )\n",
        "\n",
        "# Show the user a sample of the dataset for reference\n",
        "print(f\"X-train sample: {X_train.head()}\")\n",
        "print(f\"y-train sample: {y_train.head()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_J8fBrWEQqS",
        "outputId": "370003a3-90b9-4c00-a827-eff3c53599ec"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X-train sample:      Year  Month  Composite_HPI  Single_Family_Benchmark  \\\n",
            "33   2007     10          134.1                 343000.0   \n",
            "71   2010     12          139.9                 358500.0   \n",
            "154  2017     11          229.0                 594500.0   \n",
            "205  2022      2          355.7                 943800.0   \n",
            "106  2013     11          160.2                 417300.0   \n",
            "\n",
            "     One_Storey_Benchmark  Two_Storey_Benchmark  Townhouse_Benchmark  \\\n",
            "33               284700.0              388500.0             277200.0   \n",
            "71               283400.0              413900.0             288800.0   \n",
            "154              442100.0              703900.0             466200.0   \n",
            "205              722200.0             1099800.0             754200.0   \n",
            "106              322400.0              486400.0             312300.0   \n",
            "\n",
            "     Apartment_Benchmark  \n",
            "33              239700.0  \n",
            "71              251900.0  \n",
            "154             402500.0  \n",
            "205             567500.0  \n",
            "106             266400.0  \n",
            "y-train sample: 33     318700.0\n",
            "71     332600.0\n",
            "154    544400.0\n",
            "205    845400.0\n",
            "106    380900.0\n",
            "Name: Composite_Benchmark, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Basic Model"
      ],
      "metadata": {
        "id": "6-ra1oSvFQAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plant the seeds to grow a basic forest\n",
        "forest_model = RandomForestRegressor(random_state = 0)"
      ],
      "metadata": {
        "id": "V4xx31WXFSA7"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper Parameters"
      ],
      "metadata": {
        "id": "J3lY-4ikFInW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup parameters\n",
        "param_dist = {'n_estimators': [\n",
        "    50,\n",
        "    100,\n",
        "    150,\n",
        "    200,\n",
        "    250,\n",
        "    300,\n",
        "    350,\n",
        "    400,\n",
        "    450,\n",
        "    500,\n",
        "    550,\n",
        "    600,\n",
        "    650,\n",
        "    700,\n",
        "    750\n",
        "    ]}\n",
        "\n",
        "# Initialize the random search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator = forest_model,\n",
        "    param_distributions = param_dist,\n",
        "    n_iter = 15,\n",
        "    cv = 5,\n",
        "    scoring = 'neg_mean_squared_error',\n",
        "    random_state = 0\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Store the best selection for later\n",
        "selected_params = random_search.best_params_\n",
        "\n",
        "# Show the user the best parameter\n",
        "print(f\"Selected Param: {selected_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UiXqP6MFLNU",
        "outputId": "12bd67cb-0e9d-4854-c0e8-a83de7a9d36c"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Param: {'n_estimators': 350}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model Set up"
      ],
      "metadata": {
        "id": "-VHyDkhYGgda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-Initialize Random Forest using the selected Params\n",
        "final_forest = RandomForestRegressor(\n",
        "    n_estimators = selected_params['n_estimators'],\n",
        "    random_state = 0\n",
        ")\n",
        "\n",
        "# Fit data to the new model\n",
        "final_forest.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction to get a score to check\n",
        "score_predict = final_forest.predict(X_test)"
      ],
      "metadata": {
        "id": "WsDwMAUHGjDO"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scoring"
      ],
      "metadata": {
        "id": "ygb03VUgXQFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass in your y test data and check it against your y predicted data\n",
        "mse = mean_squared_error(y_test, score_predict)\n",
        "\n",
        "# Share with the world!\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcqrogvgXR_4",
        "outputId": "749bbe6e-c647-4eb7-831f-d0c36b86c70d"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 13609729.002661912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Future Data"
      ],
      "metadata": {
        "id": "4Wo7sjqUHMZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset for future dates\n",
        "future_dates = pd.date_range(start='2024-01-01', periods=120, freq='M')\n",
        "future_data = pd.DataFrame({'Date': future_dates})\n",
        "\n",
        "# # Create future dataset with date features\n",
        "future_data['Year'] = future_data['Date'].dt.year\n",
        "future_data['Month'] = future_data['Date'].dt.month\n",
        "\n",
        "both_sets = [dataset, future_data]\n",
        "\n",
        "# Add the old data to the new set\n",
        "new_data = pd.concat(both_sets)"
      ],
      "metadata": {
        "id": "_HXzmqbYHPL9"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression to backfill the Nulls"
      ],
      "metadata": {
        "id": "HhaH7gL2SxGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the missing data columns\n",
        "missing_data_cols = [\n",
        "    'Composite_HPI',\n",
        "    'Single_Family_Benchmark',\n",
        "    'One_Storey_Benchmark',\n",
        "    'Two_Storey_Benchmark',\n",
        "    'Townhouse_Benchmark',\n",
        "    'Apartment_Benchmark'\n",
        "]\n",
        "\n",
        "# Iterate over the list of columns and predict the future for each\n",
        "for col in missing_data_cols:\n",
        "  print(col)\n",
        "\n",
        "  missing_data = new_data[\n",
        "      new_data[col].isnull()\n",
        "      ]\n",
        "\n",
        "  completed_data = new_data.dropna(subset = [col])\n",
        "\n",
        "  # Break out the X and y sets for the backfill model\n",
        "  # X_backfill = completed_data.select_dtypes(\n",
        "  #     include = ['float64', 'int64']\n",
        "  #     ).drop(\n",
        "  #         columns = missing_data_cols\n",
        "  #         )\n",
        "\n",
        "  X_backfill = new_data[col].dropna()\n",
        "  X_backfill = X_backfill.array.reshape(-1, 1)\n",
        "\n",
        "  y_backfill = completed_data[col]\n",
        "\n",
        "  # Instatiate a Linear Regression Model\n",
        "  backfill_model = LinearRegression()\n",
        "\n",
        "  # Fit the backfill model\n",
        "  backfill_model.fit(X_backfill, y_backfill)\n",
        "\n",
        "  # Predict what the missing values would be\n",
        "  X_missing = missing_data[col].replace([np.nan], 0)\n",
        "  X_missing = X_missing.array.reshape(-1, 1)\n",
        "\n",
        "  imputer = SimpleImputer(\n",
        "      strategy = 'mean',\n",
        "      # missing_values=np.nan\n",
        "      )\n",
        "\n",
        "  X_missing_transformed = imputer.fit_transform(X_missing)\n",
        "\n",
        "  predicted_future_data = backfill_model.predict(X_missing_transformed)\n",
        "\n",
        "  print(predicted_future_data)\n",
        "\n",
        "  # # Add the predicted data to the OG new_data object\n",
        "  # new_data.iloc[\n",
        "  #     missing_data.index,\n",
        "  #     col\n",
        "  #     ] = predicted_future_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M9UG1stSz2X",
        "outputId": "d98a9925-127c-41b9-b1b7-0d7d208ce337"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Composite_HPI\n",
            "[2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14\n",
            " 2.84217094e-14 2.84217094e-14 2.84217094e-14 2.84217094e-14]\n",
            "Single_Family_Benchmark\n",
            "[2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10\n",
            " 2.91038305e-10 2.91038305e-10 2.91038305e-10 2.91038305e-10]\n",
            "One_Storey_Benchmark\n",
            "[-5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11\n",
            " -5.82076609e-11 -5.82076609e-11 -5.82076609e-11 -5.82076609e-11]\n",
            "Two_Storey_Benchmark\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Townhouse_Benchmark\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Apartment_Benchmark\n",
            "[5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11\n",
            " 5.82076609e-11 5.82076609e-11 5.82076609e-11 5.82076609e-11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict Outcome"
      ],
      "metadata": {
        "id": "a_YibEsHINFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab features for prediction\n",
        "features_for_prediction = new_data[[\n",
        "    'Year',\n",
        "    'Month',\n",
        "    'Composite_HPI',\n",
        "    'Single_Family_Benchmark',\n",
        "    'One_Storey_Benchmark',\n",
        "    'Two_Storey_Benchmark',\n",
        "    'Townhouse_Benchmark',\n",
        "    'Apartment_Benchmark'\n",
        "    ]]\n",
        "\n",
        "# Print a sample of the normalized future dataset\n",
        "print(f\"Normalized Future Data:\\n {features_for_prediction}\")\n",
        "\n",
        "# Predict future prices\n",
        "future_predictions = final_forest.predict(features_for_prediction)\n",
        "\n",
        "print(future_predictions)\n",
        "\n",
        "# Add predicted prices to future_data\n",
        "future_data['Composite_Benchmark_Predicted'] = future_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XUaVLvuzITjf",
        "outputId": "5afa7e19-bfcd-4437-c69a-47594a9d49d8"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Future Data:\n",
            "      Year  Month  Composite_HPI  Single_Family_Benchmark  \\\n",
            "0    2005      1          100.0                 257600.0   \n",
            "1    2005      2          101.0                 260100.0   \n",
            "2    2005      3          102.1                 263000.0   \n",
            "3    2005      4          103.1                 265800.0   \n",
            "4    2005      5          103.8                 267500.0   \n",
            "..    ...    ...            ...                      ...   \n",
            "115  2033      8            NaN                      NaN   \n",
            "116  2033      9            NaN                      NaN   \n",
            "117  2033     10            NaN                      NaN   \n",
            "118  2033     11            NaN                      NaN   \n",
            "119  2033     12            NaN                      NaN   \n",
            "\n",
            "     One_Storey_Benchmark  Two_Storey_Benchmark  Townhouse_Benchmark  \\\n",
            "0                205000.0              300000.0             199600.0   \n",
            "1                207400.0              302700.0             201400.0   \n",
            "2                210000.0              305700.0             202900.0   \n",
            "3                212700.0              308600.0             204500.0   \n",
            "4                214500.0              310100.0             205900.0   \n",
            "..                    ...                   ...                  ...   \n",
            "115                   NaN                   NaN                  NaN   \n",
            "116                   NaN                   NaN                  NaN   \n",
            "117                   NaN                   NaN                  NaN   \n",
            "118                   NaN                   NaN                  NaN   \n",
            "119                   NaN                   NaN                  NaN   \n",
            "\n",
            "     Apartment_Benchmark  \n",
            "0               172400.0  \n",
            "1               174600.0  \n",
            "2               176400.0  \n",
            "3               177800.0  \n",
            "4               179500.0  \n",
            "..                   ...  \n",
            "115                  NaN  \n",
            "116                  NaN  \n",
            "117                  NaN  \n",
            "118                  NaN  \n",
            "119                  NaN  \n",
            "\n",
            "[349 rows x 8 columns]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-229a7a5af785>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Predict future prices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfuture_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_for_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    600\u001b[0m         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n\u001b[1;32m    601\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No support for np.int64 index based sparse matrices\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graphs"
      ],
      "metadata": {
        "id": "MmZ1Rys8Id9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the predicted Composite Benchmark prices over time\n",
        "plt.plot(\n",
        "    future_data['Date'],\n",
        "    future_data['Composite_Benchmark_Predicted'],\n",
        "    label='Predicted Prices'\n",
        "    )\n",
        "\n",
        "# Set up the axis\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title('Next Ten Years Predicted Composite Benchmark Prices')\n",
        "\n",
        "# Make the graph look nice\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the graph for a rainy day\n",
        "plt.savefig('PredictionGraph.svg', format='svg', bbox_inches='tight')\n",
        "\n",
        "# Display the graph to the end user\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FvB9BpccIgA-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}